from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import time
import signal
import sys

print("=== Improved Kafka Spark Streaming ===")

# –ì–ª–æ–±–∞–ª—å–Ω–∞ –∑–º—ñ–Ω–Ω–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –ø–æ—Ç–æ–∫–æ–º
is_running = True

def signal_handler(sig, frame):
    global is_running
    print("\nüõë Received interrupt signal. Stopping gracefully...")
    is_running = False

# –†–µ—î—Å—Ç—Ä—É—î–º–æ –æ–±—Ä–æ–±–Ω–∏–∫ —Å–∏–≥–Ω–∞–ª—ñ–≤
signal.signal(signal.SIGINT, signal_handler)

spark = SparkSession.builder \
    .appName("KafkaStreamProcessor") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.7") \
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true") \
    .config("spark.sql.adaptive.enabled", "false") \
    .config("spark.streaming.stopGracefullyOnShutdown", "true") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
print(f"‚úÖ Spark {spark.version} started")

try:
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —É–º–æ–≤–∏ –∞–ª–µ—Ä—Ç—ñ–≤
    alerts_df = spark.read.csv("data/alerts_conditions.csv", header=True, inferSchema=True)
    print("üìã Alerts conditions loaded")
    alerts_df.show()
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Ç—ñ–∫ –∑ Kafka
    kafka_stream = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "77.81.230.104:9092") \
        .option("kafka.security.protocol", "SASL_PLAINTEXT") \
        .option("kafka.sasl.mechanism", "PLAIN") \
        .option("kafka.sasl.jaas.config", 
                'org.apache.kafka.common.security.plain.PlainLoginModule required '
                'username="admin" password="VawEzo1ikLtrA8Ug8THa";') \
        .option("subscribe", "building_sensors_greenmoon") \
        .option("startingOffsets", "earliest") \
        .option("maxOffsetsPerTrigger", "10") \
        .load()
    
    print("üîå Connected to Kafka successfully")
    
    # –°—Ö–µ–º–∞ –¥–ª—è JSON –¥–∞–Ω–∏—Ö
    json_schema = StructType([
        StructField("sensor_id", IntegerType()),
        StructField("timestamp", StringType()),
        StructField("temperature", IntegerType()),
        StructField("humidity", IntegerType())
    ])
    
    # –ü–∞—Ä—Å–∏–º–æ JSON
    parsed_stream = kafka_stream.select(
        from_json(col("value").cast("string"), json_schema).alias("data")
    ).select(
        col("data.sensor_id"),
        col("data.timestamp"),
        col("data.temperature"),
        col("data.humidity")
    )
    
    # –ê–≥—Ä–µ–≥–∞—Ü—ñ—ó –ø–æ –≤—ñ–∫–Ω—É
    windowed_avg = parsed_stream \
        .withColumn("ts", from_unixtime(col("timestamp").cast("double")).cast(TimestampType())) \
        .withWatermark("ts", "30 seconds") \
        .groupBy(
            window(col("ts"), "1 minute", "30 seconds")
        ) \
        .agg(
            avg("temperature").alias("avg_temp"),
            avg("humidity").alias("avg_humidity"),
            count("*").alias("message_count")
        )
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∞–ª–µ—Ä—Ç—ñ–≤
    alerts = windowed_avg.crossJoin(alerts_df) \
        .filter(
            (col("avg_temp").between(col("temperature_min"), col("temperature_max"))) |
            (col("avg_humidity").between(col("humidity_min"), col("humidity_max")))
        ) \
        .select(
            col("window.start").alias("window_start"),
            col("window.end").alias("window_end"),
            col("avg_temp"),
            col("avg_humidity"),
            col("message_count"),
            col("code"),
            col("message"),
            current_timestamp().alias("alert_timestamp")
        )
    
    print("üöÄ Starting streaming queries...")
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ç—ñ–∫ –¥–ª—è –≤–∏–≤–æ–¥—É –≤ –∫–æ–Ω—Å–æ–ª—å
    console_query = alerts \
        .writeStream \
        .outputMode("update") \
        .format("console") \
        .option("truncate", "false") \
        .option("numRows", 10) \
        .option("checkpointLocation", "/tmp/spark-kafka-checkpoint-console") \
        .start()
    
    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –∑–∞–ø–∏—Å—É –≤ Kafka
    kafka_output = alerts.select(
        col("code").alias("key"),
        to_json(
            struct(
                col("window_start"),
                col("window_end"), 
                col("avg_temp"),
                col("avg_humidity"),
                col("message_count"),
                col("code"),
                col("message"),
                col("alert_timestamp")
            )
        ).alias("value")
    )
    
    # –ó–∞–ø–∏—Å –≤ Kafka
    kafka_query = kafka_output \
        .writeStream \
        .outputMode("update") \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "77.81.230.104:9092") \
        .option("kafka.security.protocol", "SASL_PLAINTEXT") \
        .option("kafka.sasl.mechanism", "PLAIN") \
        .option("kafka.sasl.jaas.config", 
                'org.apache.kafka.common.security.plain.PlainLoginModule required '
                'username="admin" password="VawEzo1ikLtrA8Ug8THa";') \
        .option("topic", "building_alerts_greenmoon") \
        .option("checkpointLocation", "/tmp/kafka-output-checkpoint") \
        .start()
    
    print("‚úÖ Both streams started successfully!")
    print("üìä Console output and Kafka writing active")
    print("üì® Alerts being written to: building_alerts_greenmoon")
    print("üí° Make sure generator.py is running in another terminal")
    print("‚èπÔ∏è  Press Ctrl+C to stop")
    
    # –ß–µ–∫–∞—î–º–æ –ø–æ–∫–∏ –ø–æ—Ç—ñ–∫ –∞–∫—Ç–∏–≤–Ω–∏–π —ñ –º–∏ –Ω–µ –æ—Ç—Ä–∏–º–∞–ª–∏ —Å–∏–≥–Ω–∞–ª –∑—É–ø–∏–Ω–∫–∏
    while is_running and (console_query.isActive or kafka_query.isActive):
        time.sleep(1)
        status = console_query.status
    
    print("üõë Stopping streams...")
    console_query.stop()
    kafka_query.stop()
    
except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()

finally:
    spark.stop()
    print("‚úÖ Spark session stopped cleanly")
